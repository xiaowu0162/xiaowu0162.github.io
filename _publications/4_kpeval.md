---
title: "KPEval: Towards Fine-Grained Semantic-Based Keyphrase Evaluation"
collection: publications
Authors: '<b>Di Wu</b>, Da Yin, and Kai-Wei Chang.'
date: 05/2024
venue: 'ACL (Findings)'
paperurl: 'https://arxiv.org/abs/2303.15422'
codeurl: 'https://github.com/uclanlp/KPEval'
presentationurl: 'https://drive.google.com/file/d/1tOSDFN24Z_ipZFS1d42xFC72yrUlLf_u/view?usp=sharing'
posterurl: 'https://drive.google.com/file/d/1Pv1zOFoOHSLHzijHnLuiqT9SbKxPDrHm/view?usp=sharing'
excerpt: ''
topic: 'keyphrase'
selected: 'true'
---
---
<a href='https://arxiv.org/pdf/2303.15422.pdf' target="_blank">[Download Paper]</a><a href='https://github.com/uclanlp/KPEval' target="_blank">[Source Code]</a>

<p align="justify">
Despite the significant advancements in keyphrase extraction and keyphrase generation methods, the predominant approach for evaluation mainly relies on exact matching with human references. This scheme fails to recognize systems that generate keyphrases semantically equivalent to the references or diverse keyphrases that carry practical utility. To better assess the capability of keyphrase systems, we propose KPEval, a comprehensive evaluation framework consisting of four critical aspects: reference agreement, faithfulness, diversity, and utility. For each aspect, we design semantic-based metrics to reflect the evaluation objectives. Meta-evaluation studies demonstrate that our evaluation strategy correlates better with human preferences compared to a range of previously used metrics. Using KPEval, we re-evaluate 20 keyphrase systems and discover that (1) the best model differs depending on the evaluated aspect; (2) the utility in downstream tasks does not always correlate with reference agreement; and (3) large language models exhibit strong performance with few-shot prompting, especially under reference-free evaluation.
</p>
